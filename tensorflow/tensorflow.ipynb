{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.15 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from -r ./requirements.txt (line 1)) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (1.26.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (4.10.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorflow==2.15->-r ./requirements.txt (line 1)) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow==2.15->-r ./requirements.txt (line 1)) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/alpsilva/miniconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15->-r ./requirements.txt (line 1)) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    " \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent: Greeting\n",
      "text: ['Olá', 'Opa', 'Bom dia', 'Boa tarde', 'Boa noite', 'Olá, está aí?', 'Oi', 'Tudo bem?', 'dale']\n",
      "responses: ['Olá! Como posso te ajudar?']\n",
      "extension: {'function': '', 'entities': False, 'responses': []}\n",
      "context: {'in': '', 'out': '', 'clear': False}\n",
      "entityType: NA\n",
      "entities: []\n"
     ]
    }
   ],
   "source": [
    "with open('./data/simple_intent.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# print(data.keys())\n",
    "# print(type(data['intents']))\n",
    "# print(len(data['intents']))\n",
    "\n",
    "training_intents = data['intents']\n",
    "\n",
    "for key, value in training_intents[0].items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(line):\n",
    "    cleaned_line = ''\n",
    "    for char in line:\n",
    "        if char.isalpha():\n",
    "            cleaned_line += char\n",
    "        else:\n",
    "            cleaned_line += ' '\n",
    "    cleaned_line = ' '.join(cleaned_line.split())\n",
    "    return cleaned_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "#list of intents\n",
    "intents = []                                            \n",
    "unique_intents = []\n",
    "#all text data to create a corpus\n",
    "text_input= []    \n",
    "#dictionary mapping intent with appropriate response\n",
    "response_for_intent = {}                                \n",
    "for intent in training_intents:\n",
    "    intent_name = intent['intent']\n",
    "    intent_texts = intent['text']\n",
    "    #list of unique intents\n",
    "    \n",
    "    if intent_name not in unique_intents:            \n",
    "        unique_intents.append(intent_name)  \n",
    "    for text in intent_texts:\n",
    "        #cleaning is done before adding text to corpus\n",
    "        text_input.append(clean(text))                    \n",
    "        intents.append(intent_name)\n",
    "    if intent_name not in response_for_intent:\n",
    "        response_for_intent[intent_name] = [] \n",
    "    for response in intent['responses']:\n",
    "        response_for_intent[intent_name].append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Embedding\n",
    "Now, our data is ready to be tokenized, with the help of inbuilt TensorFlow tokenizer, We can make both tokenization and embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input Sequence: (29, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  2],\n",
       "       [ 0,  0,  8],\n",
       "       [ 0,  9, 10],\n",
       "       [ 0,  3, 11],\n",
       "       [ 0,  3, 12]], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters='',oov_token='<unk>')\n",
    "tokenizer.fit_on_texts(text_input)\n",
    "sequences = tokenizer.texts_to_sequences(text_input)\n",
    "padded_sequences = pad_sequences(sequences, padding='pre')\n",
    "print('Shape of Input Sequence:',padded_sequences.shape)\n",
    "padded_sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "Neural network cannot process sentences, so numerical representation of sentences have to be provided to it, this is done by doing Feature Extraction, for that we map all words with their indexes and create a matrix mapping it to its category (intent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Intents : 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'Greeting', 1: 'Thanks', 2: 'Cancel'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   \n",
    "intent_to_index = {}\n",
    "categorical_target = []\n",
    "index = 0\n",
    " \n",
    "for intent in intents:\n",
    "    if intent not in intent_to_index:\n",
    "        intent_to_index[intent] = index\n",
    "        index += 1\n",
    "    categorical_target.append(intent_to_index[intent])\n",
    " \n",
    "num_classes = len(intent_to_index)\n",
    "print('Number of Intents :',num_classes)\n",
    " \n",
    "# Convert intent_to_index to index_to_intent\n",
    "index_to_intent = {index: intent for intent, index in intent_to_index.items()}\n",
    "index_to_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Ca (29, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_vec = tf.keras.utils.to_categorical(\n",
    "    categorical_target, \n",
    "    num_classes=num_classes,\n",
    "    dtype='int32'\n",
    ")\n",
    " \n",
    "print('Shape of Ca',categorical_vec.shape)\n",
    "categorical_vec[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "First step of building a model is defining hyperparameters, in simple words they are settings which we predefine before training our model. They include parameters like no. of epochs, embedding dimensions, size of vocabulary, and target length. They can be adjusted accordingly, to increase performance of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dimension :3,\n",
      "Output Dimension :3\n"
     ]
    }
   ],
   "source": [
    "# These hyper parameters could be the target of an optmization study   \n",
    "epochs=100\n",
    "embed_dim=300\n",
    "lstm_num=50\n",
    "\n",
    "output_dim=categorical_vec.shape[1]\n",
    "input_dim=len(unique_intents)\n",
    "print(\"Input Dimension :{},\\nOutput Dimension :{}\".format(input_dim,output_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As both input dimension and output dimension are same, we can proceed with building our model.\n",
    "\n",
    "Now, we can define the architecture of our neural network using TensorFlow. A common model for intent recognition is the recurrent neural network (RNN) or its variant, the long short-term memory (LSTM) network. These networks can handle sequential data, such as sentences, effectively. We can also use pre-trained models like BERT or GPT to achieve better performance.\n",
    "\n",
    "Here we are using a RNN, by using ‘Sequential’ model from TensorFlow’s Keras API. It consists of an embedding layer, an LSTM layer for sequence processing, and two dense layers for classification. You can see model summary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 300)         10500     \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 100)               140400    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156103 (609.78 KB)\n",
      "Trainable params: 156103 (609.78 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, embed_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_num, dropout=0.1)),  \n",
    "    tf.keras.layers.Dense(lstm_num, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(output_dim, activation='softmax')\n",
    "])\n",
    " \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)  \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb5007d0f90>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_sequences, categorical_vec, epochs=epochs, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7830 - accuracy: 0.7647\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    (\"Oi\", \"Greeting\"),\n",
    "    (\"Obrigado\", \"Thanks\"),\n",
    "    (\"Grato\", \"Thanks\"),\n",
    "    (\"Valeu\", \"Thanks\"),\n",
    "    (\"vlw\", \"Thanks\"),\n",
    "    (\"Opa\", \"Greeting\"),\n",
    "    (\"Dale\", \"Greeting\"),\n",
    "    (\"oioi\", \"Greeting\"),\n",
    "    (\"ei\", \"Greeting\"),\n",
    "    (\"Grata!\", \"Thanks\"),\n",
    "    (\"Muito obrigada, de verdade\", \"Thanks\"),\n",
    "    (\"Cancelar\", \"Cancel\"),\n",
    "    (\"Deixa para outro dia\", \"Cancel\"),\n",
    "    (\"encerrar\", \"Cancel\"),\n",
    "    (\"cancela\", \"Cancel\"),\n",
    "    (\"Esquece\", \"Cancel\"),\n",
    "    (\"encerra\", \"Cancel\")\n",
    "]\n",
    "\n",
    "test_text_inputs, test_intents = [], []\n",
    "for test in tests:\n",
    "    test_text_inputs.append(test[0])\n",
    "    test_intents.append(test[1])\n",
    " \n",
    "test_sequences = tokenizer.texts_to_sequences(test_text_inputs)\n",
    "test_padded_sequences = pad_sequences(test_sequences,  padding='pre')\n",
    "test_labels = np.array([unique_intents.index(intent) for intent in test_intents])\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=num_classes)\n",
    "loss, accuracy = model.evaluate(test_padded_sequences, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.43986425 0.2672869  0.29284883]], shape=(1, 3), dtype=float32)\n",
      "[0]\n",
      "Response: Olá! Como posso te ajudar?\n",
      "Intent: Greeting\n"
     ]
    }
   ],
   "source": [
    "def response(sentence):\n",
    "    sent_tokens = []\n",
    "    # Split the input sentence into words\n",
    "    words = sentence.split()\n",
    "    # Convert words to their corresponding word indices\n",
    "    for word in words:                                           \n",
    "        if word in tokenizer.word_index:\n",
    "            sent_tokens.append(tokenizer.word_index[word])\n",
    "        else:\n",
    "            # Handle unknown words\n",
    "            sent_tokens.append(tokenizer.word_index['<unk>'])\n",
    "    sent_tokens = tf.expand_dims(sent_tokens, 0)\n",
    "    #predict numerical category\n",
    "    pred = model(sent_tokens)\n",
    "    print(pred)\n",
    "    #category to intent\n",
    "    pred_class = np.argmax(pred.numpy(), axis=1)    \n",
    "    print(pred_class)            \n",
    "    # random response to that intent\n",
    "    return random.choice(\n",
    "        response_for_intent[index_to_intent[pred_class[0]]]), index_to_intent[pred_class[0]]\n",
    "\n",
    "   \n",
    "query = \"Obrigado\"\n",
    "bot_response, typ = response(query)\n",
    "print(f\"Response: {bot_response}\")\n",
    "print(f\"Intent: {typ}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
